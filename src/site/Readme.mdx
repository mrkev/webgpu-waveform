import { GPUWaveform, useWaveformRenderer } from "../index";
import { audioContext, loadSound, usePromise } from "./utils";
import { Example, Main, Example1, Example2, Example3 } from "./Examples"
import { useRef, useEffect } from 'react'
import GitHubButton from 'react-github-btn'

export function H3ID({ id, children }) {
  return (
    <a href={`#${id}`}>
      <h3 id={id}>{children}</h3>
    </a>
  )
}

# webgpu-waveform

Render waveforms to `<canvas />` using [WebGPU](https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API)

<p>
  <a href="https://github.com/mrkev/webgpu-waveform"><i className="ri-github-fill" ></i> View on github</a>
</p>


## Usage

There's three ways to use this library:
- as a class: <a href="#GPUWaveformRenderer">`GPUWaveformRenderer`</a> (vanilla JS)
- as a hook: <a href="#useWaveformRenderer">`useWaveformRenderer`</a> (React)
- as a component: <a href="#GPUWaveform">`GPUWaveform`</a> (React)

_Note: Examples here use a waveform that looks like a cow. Source: [Japhy Riddle](https://www.youtube.com/watch?v=qeUAHHPt-LY)_

<Example render={audioBuffer => <Main audioBuffer={audioBuffer} />} />

<p>
<details>
  <summary style={{background: 'black'}}>Source</summary>
```tsx
export function Main({ audioBuffer }: { audioBuffer: AudioBuffer }) {
  const [offsetFr, setOffsetFr] = useState(0);
  const [frPerPx, setFrPerPx] = useState(441);

  return (
    <div style={{ display: "flex", flexDirection: "column" }}>
      <div style={{ display: "flex", flexDirection: "row" }}>
        Offset:{" "}
        <input
          type="range"
          min={0}
          max={audioBuffer.length}
          value={offsetFr}
          onChange={(v) => {
            const value = parseInt(v.target.value);
            setOffsetFr(value);
          }}
        ></input>{" "}
        {offsetFr} frames
      </div>
      <div style={{ display: "flex", flexDirection: "row" }}>
        Scale:{" "}
        <input
          type="range"
          min={1}
          max={1764}
          value={frPerPx}
          onChange={(v) => {
            const value = parseInt(v.target.value);
            setFrPerPx(value);
          }}
        ></input>{" "}
        {frPerPx} frames / pixel
      </div>

      <GPUWaveform
        audioBuffer={audioBuffer}
        scale={frPerPx}
        offset={offsetFr}
        height={50}
      />
    </div>
  );
}
```
</details>
</p>

<br />

<H3ID id={'GPUWaveformRenderer'}>Using the `GPUWaveformRenderer` class</H3ID>

The class `GPUWaveformRenderer` is initialized using the static method `.create(...)`. It has the following definition:

```typescript
static async create(
  canvas: HTMLCanvasElement,
  channelData: Float32Array
): GPUWaveformRenderer
```

It takes in the following arguments:

- `canvas: HTMLCanvasElement` — the canvas element to render to
- `channelData: Float32Array` — the array of PCM samples to render

*Example:*

```javascript
async function example(canvas, audioBuffer) {
  const channelData = audioBuffer.getChannelData(0);
  const renderer = await GPUWaveformRenderer.create(canvas, channelData);

  renderer?.render(800, 0, canvas.width, canvas.height);
}
```

*Result:*

<Example render={audioBuffer => <Example1 audioBuffer={audioBuffer} />} />

<H3ID id={'useWaveformRenderer'}>Using the `useWaveformRenderer` hook</H3ID>

The hook `useWaveformRenderer` has the following signature:

```typescript
function useWaveformRenderer(
  canvasRef: React.RefObject<HTMLCanvasElement>,
  audioBuffer: AudioBuffer
): RendererStatus;
```

It takes in the following arguments:

- `canvasRef: React.RefObject<HTMLCanvasElement>` — the canvas element to render to
- `audioBuffer: AudioBuffer` — the buffer to render

And returns an object of the following type:

```typescript
type RendererStatus =
  | { status: "initializing" }
  | { status: "error"; error: any }
  | { status: "ready";
      instance: GPUWaveformRenderer;
    };
```

The objects are returned during the following stages of initialization:

- `{ status: "initializing" }` — during setup, when connecting to the GPU device
- `{ status: "error"; error: any }` — if an error happens at initalization
- `{ status: "ready"; instance: GPUWaveformRenderer }` — if the webgpu device could be initialized, setup was successful, and a renderer for `audioBuffer` on `canvas` could be successfully created.

**Example:**

```javascript
function Example({audioBuffer, width, height}) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const renderer = useWaveformRenderer(canvasRef, audioBuffer);

  useEffect(() => {
    if (renderer.status !== "ready") {
      return;
    }

    renderer?.render(audioBuffer.length / width, 0, width, height);
  }, [renderer, audioBuffer, width, height]);

  return <canvas ref={canvasRef} width={width} height={height} />;
};

```

**Result:**

<Example render={audioBuffer => <Example2 audioBuffer={audioBuffer} width={300} height={100} />} />

<H3ID id={'GPUWaveform'}>Using the `GPUWaveform` component</H3ID>

The component `GPUWaveform` takes the following properties:

- `audioBuffer: AudioBuffer;` — the buffer to render
- `scale?: number;` — the "zoom" level. Namely, number of samples per pixel in the x axis
- `offset?: number;` — the number of samples to skip from the beggining of the buffer
- ...and all the props of `React.CanvasHTMLAttributes<HTMLCanvasElement>` — these are passed directly to the rendered canvas

Example:

```javascript
export function Example({audioBuffer}) {
  return (
    <GPUWaveform
      audioBuffer={audioBuffer}
      scale={800}
      width={300}
      height={100}
    />
  )
}
```

Result:

<Example render={audioBuffer => <Example3 audioBuffer={audioBuffer} />} />


<footer>
    Built by Kevin Chavez · <a href="https://twitter.com/aykev"><i className="ri-twitter-fill"></i></a> <a href="https://github.com/mrkev"><i className="ri-github-fill" ></i></a>
</footer>