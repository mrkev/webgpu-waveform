import React, { useEffect } from "react";
import GitHubButton from "react-github-btn";
import {
  GPUWaveform,
  useWaveformRenderer,
} from "webgpu-waveform-react";
import { Example, Example1, Example2, Example3 } from "./Examples";
import { Playground } from "./Playground";
import { audioContext, loadSound, usePromise } from "./utils";

export function HID({ h = "h4", id, children }) {
  return (
    <a href={`#${id}`}>
      {React.createElement(
        h,
        { id },
        ...(Array.isArray(children) ? children : [children])
      )}
    </a>
  );
}

# webgpu-waveform

Render waveforms to `<canvas />` using [WebGPU](https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API). It's faster.

- <a href="https://github.com/mrkev/webgpu-waveform">
    <i className="ri-github-fill"></i> view on github
  </a>
- [<i className="ri-npmjs-fill"></i> webgpu-waveform on NPM](https://www.npmjs.com/package/webgpu-waveform)
- [<i className="ri-npmjs-fill"></i> webgpu-waveform-react on NPM](https://www.npmjs.com/package/webgpu-waveform-react)

_Note: Examples here use a waveform that looks like a cow. Source: [Japhy Riddle](https://www.youtube.com/watch?v=qeUAHHPt-LY)_

<Example render={(audioBuffer) => <Playground audioBuffer={audioBuffer} />} />

<details style={{paddingLeft: '11%'}}>
  <summary>Source</summary>
```tsx
export function Main({ audioBuffer }: { audioBuffer: AudioBuffer }) {
  const [offsetFr, setOffsetFr] = useState(0);
  const [frPerPx, setFrPerPx] = useState(441);
  const [color, setColor] = useState("#00FF00");

  return (
    <div style={{ display: "flex", flexDirection: "column" }}>
      <div style={{ display: "flex", flexDirection: "row" }}>
        Color:{" "}
        <input
          type="color"
          value={color}
          onChange={(e) => setColor(e.target.value)}
        />
      </div>
      <div style={{ display: "flex", flexDirection: "row" }}>
        Offset:{" "}
        <input
          type="range"
          min={0}
          max={audioBuffer.length}
          value={offsetFr}
          onChange={(v) => {
            const value = parseInt(v.target.value);
            setOffsetFr(value);
          }}
        ></input>{" "}
        {offsetFr} frames
      </div>
      <div style={{ display: "flex", flexDirection: "row" }}>
        Scale:{" "}
        <input
          type="range"
          min={1}
          max={1764}
          value={frPerPx}
          onChange={(v) => {
            const value = parseInt(v.target.value);
            setFrPerPx(value);
          }}
        ></input>{" "}
        {frPerPx} frames / pixel
      </div>

      <GPUWaveform
        audioBuffer={audioBuffer}
        scale={frPerPx}
        offset={offsetFr}
        height={50}
        color={color}
      />
    </div>
  );
}
```
</details>

## Usage

There's 2 packages:

- [`webgpu-waveform`](#pacakge:webgpu-waveform) (vanilla JS/TS)
- [`webgpu-waveform-react`](#pacakge:webgpu-waveform-react) (to use with [React](https://react.dev/))

Both are distributed to be ESM and UMD compatible, and include TypeScript type declarations.


<HID h='h3' id='pacakge:webgpu-waveform'>`webgpu-waveform`</HID>

Install from the npm registry:

```bash
npm i webgpu-waveform
````

Use `GPUWaveformRenderer.create(...)` to create a new `GPUWaveformRenderer`.

```typescript
static async create(
  channelData: Float32Array,
): GPUWaveformRenderer
```

It takes in the following argument:

- `channelData: Float32Array` — the array of PCM samples to render

Finally, use `.render` on the `GPUWaveformRenderer` to render the waveform. It has the following signature:

```typescript
render(
  destination: HTMLCanvasElement | OffscreenCanvas | GPUCanvasContext,
  scale: number,
  offset: number,
  color?: string | [r: number, g: number, b: number, a: number],
);
```

Where:

- `destination: React.RefObject<HTMLCanvasElement>` is the canvas or context to render to
- `scale: number` is the number of frames (pulses) rendered per pixel
- `offset: number` is the first frame (pulse) that will be rendered
- `color?: string | [r: number, g: number, b: number, a: number]` is the color of the waveform

> Note: you can set the background color directly on the destination `<canvas />` element via CSS

_Example:_

```javascript
import { GPUWaveformRenderer } from "webgpu-waveform";

async function example(canvas, audioBuffer) {
  const channelData = audioBuffer.getChannelData(0);
  const renderer = await GPUWaveformRenderer.create(channelData);

  renderer?.render(canvas, 800, 0);
}
```

_Result:_

<Example render={(audioBuffer) => <Example1 audioBuffer={audioBuffer} />} />

<HID h="h3" id="pacakge:webgpu-waveform-react">
  <code>webgpu-waveform-react</code>
</HID>

There's two ways to use this library:

- as a component: <a href="#GPUWaveform">`GPUWaveform`</a>
- as a hook: <a href="#useWaveformRenderer">`useWaveformRenderer`</a>

<HID id={"GPUWaveform"}>Using the `GPUWaveform` component</HID>

The component `GPUWaveform` takes the following properties:

- `audioBuffer: AudioBuffer;` — the buffer to render
- `scale?: number;` — the "zoom" level. Namely, number of samples per pixel in the x axis
- `offset?: number;` — the number of samples to skip from the beggining of the buffer
- ...and all the props of `React.CanvasHTMLAttributes<HTMLCanvasElement>` — these are passed directly to the rendered canvas

Example:

```typescript
import { GPUWaveform } from "webgpu-waveform-react";

export function ExampleComponent({
  audioBuffer,
}: {
  audioBuffer: AudioBuffer;
}) {
  return (
    <GPUWaveform
      audioBuffer={audioBuffer}
      scale={400}
      style={{
        width: 300,
        height: 100,
      }}
      color="#FFFF00"
      width={300 * devicePixelRatio}
      height={100 * devicePixelRatio}
    />
  );
}
```

Result:

<Example render={(audioBuffer) => <Example3 audioBuffer={audioBuffer} />} />

<HID id={"useWaveformRenderer"}>Using the `useWaveformRenderer` hook</HID>

The hook `useWaveformRenderer` has the following signature:

```typescript
function useWaveformRenderer(audioBuffer: AudioBuffer): RendererStatus;
```

It takes in the following argument:

- `audioBuffer: AudioBuffer` — the buffer to render

And returns an object of the following type:

```typescript
type RendererStatus =
  | { status: "initializing" }
  | { status: "error"; error: any }
  | { status: "ready"; instance: GPUWaveformRenderer };
```

The objects are returned during the following stages of initialization:

- `{ status: "initializing" }` — during setup, when connecting to the GPU device
- `{ status: "error"; error: any }` — if an error happens at initalization
- `{ status: "ready"; instance: GPUWaveformRenderer }` — if the webgpu device could be initialized, setup was successful, and a renderer for `audioBuffer` on `canvas` could be successfully created.

**Example:**

```typescript
import { useWaveformRenderer } from "webgpu-waveform-react";

function ExampleComponent({
  audioBuffer,
  width,
  height,
}: {
  audioBuffer: AudioBuffer;
  width: number;
  height: number;
}) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const renderer = useWaveformRenderer(audioBuffer);

  useEffect(() => {
    if (renderer.status !== "ready") {
      return;
    }

    renderer.instance.render(canvasRef.current!, audioBuffer.length / width, 0);
  }, [renderer, audioBuffer, width, height]);

  return <canvas ref={canvasRef} width={width} height={height} />;
}
```

**Result:**

<Example
  render={(audioBuffer) => (
    <Example2 audioBuffer={audioBuffer} width={300} height={100} />
  )}
/>

<footer>
  Built by <a href="http://aykev.dev">Kevin Chavez</a> ·{" "}
  <a href="https://twitter.com/aykev">
    <i className="ri-twitter-fill"></i>
  </a>{" "}
  <a href="https://github.com/mrkev">
    <i className="ri-github-fill"></i>
  </a>
</footer>
